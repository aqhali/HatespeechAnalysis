{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing for Hate Speech Classification\n",
    "\n",
    "### Brent Read\n",
    "\n",
    "#### This project modifies code taken from a NLP Twitter tutorial found on http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/\n",
    "\n",
    "#### The Twitter Corpus and NLTK documentation was referenced:\n",
    "http://www.nltk.org/howto/twitter.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn import cross_validation\n",
    "\n",
    "os.chdir('/home/bread/424/HateSpeech/')\n",
    "\n",
    "hateData = pd.read_csv('twitter-hate-speech-classifier-DFE-a845520.csv')\n",
    "\n",
    "#print hateData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into three sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hatefulPosts =  hateData.loc[hateData['does_this_tweet_contain_hate_speech'] == \"The tweet contains hate speech\"]\n",
    "\n",
    "hatefulLanguage =  hateData.loc[hateData['does_this_tweet_contain_hate_speech'] == \"The tweet uses offensive language but not hate speech\"]\n",
    "\n",
    "nonOffensiveTweets = hateData.loc[hateData['does_this_tweet_contain_hate_speech'] == \"The tweet is not offensive\"]\n",
    "\n",
    "listOfConfidences = hateData[\"does_this_tweet_contain_hate_speech:confidence\"]\n",
    "print listOfConfidences.std()\n",
    "print listOfConfidences.mean()\n",
    "#hatefulPosts.head()\n",
    "#hatefulLanguage.head()\n",
    "#nonOffensiveTweets.head()\n",
    "\n",
    "#Get confidence\n",
    "offensiveRate = nonOffensiveTweets[\"does_this_tweet_contain_hate_speech:confidence\"]\n",
    "print offensiveRate.std()\n",
    "print offensiveRate.mean()\n",
    "\n",
    "hatefulText = hatefulPosts['tweet_text']\n",
    "nonHatefulOffensive = hatefulLanguage['tweet_text']\n",
    "controlTweets = nonOffensiveTweets['tweet_text']\n",
    "'''\n",
    "for tweet in controlTweets:\n",
    "    print tweet\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## How many tweets are we dealing with for each of these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Hateful Tweets: %s\" % len(hatefulPosts)\n",
    "print \"Offensive Language, but not hateful: %s\" % len(hatefulLanguage)\n",
    "print \"Nonoffensive Tweets: %s\" % len(nonOffensiveTweets)\n",
    "\n",
    "print \"\\n\"\n",
    "print \"Total: %s\" % len(hateData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's be super legit and make a word cloud!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "textDump = hatefulText.to_string()\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=40, relative_scaling=.5).generate(textDump)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordcloud for offensive Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textDump = nonHatefulOffensive.to_string()\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=40, relative_scaling=.5).generate(textDump)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And for the control tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textDump = controlTweets.to_string()\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=40, relative_scaling=.5).generate(textDump)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okay, let's actually do some NLP now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "untokenizedHate = [tweet.decode('ISO-8859-1') for tweet in hatefulText]\n",
    "untokenizedOffensive = [tweet.decode('ISO-8859-1') for tweet in nonHatefulOffensive]\n",
    "untokenizedControl = [tweet.decode('ISO-8859-1') for tweet in controlTweets]\n",
    "\n",
    "tokenizedHate = [tknzr.tokenize(tweet.decode('ISO-8859-1')) for tweet in hatefulText]\n",
    "tokenizedOffensive = [tknzr.tokenize(tweet.decode('ISO-8859-1')) for tweet in nonHatefulOffensive]\n",
    "tokenizedControl = [tknzr.tokenize(tweet.decode('ISO-8859-1')) for tweet in controlTweets]\n",
    "\n",
    "print tokenizedControl[1]\n",
    "print tokenizedHate[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting a tweet corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "twitter_samples.fileids()\n",
    "\n",
    "strings = twitter_samples.strings('negative_tweets.json')\n",
    "for string in strings[:15]:\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try our first classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labeled_control_tweets = [(tweet, 'control') for tweet in tokenizedControl]\n",
    "labeled_offensive_tweets = [(tweet, 'offensive') for tweet in tokenizedOffensive]\n",
    "labeled_hateful_tweets = [(tweet, 'hateful') for tweet in tokenizedHate]\n",
    "\n",
    "print labeled_control_tweets[1]\n",
    "print labeled_hateful_tweets[1]\n",
    "print labeled_offensive_tweets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_features(wordlist):\n",
    "    wordlist = nltk.FreqDist(wordlist)\n",
    "    word_features = wordlist.keys()\n",
    "    return word_features\n",
    "def get_words_in_tweets(tweets):\n",
    "    all_words = []\n",
    "    for (words, sentiment) in tweets:\n",
    "        all_words.extend(words)\n",
    "    return all_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cheeky Filtering time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "TRAINING_SET_SIZE = 300\n",
    "\n",
    "test_tweets = []\n",
    "train_tweets = []\n",
    "\n",
    "random.shuffle(labeled_control_tweets)\n",
    "random.shuffle(labeled_hateful_tweets)\n",
    "random.shuffle(labeled_offensive_tweets)\n",
    "\n",
    "all_tweets_labeled = labeled_hateful_tweets[:1000] + labeled_control_tweets[:1000] + labeled_offensive_tweets[:1000]\n",
    "hate_vs_control_labeled = labeled_hateful_tweets[:2300] + labeled_control_tweets[:2300]\n",
    "hate_vs_offensive_labeled = labeled_hateful_tweets[:2300] + labeled_offensive_tweets[:2300]\n",
    "\n",
    "\n",
    "\n",
    "#Do the Knuth Shuffle!\n",
    "random.shuffle(all_tweets_labeled)\n",
    "\n",
    "test_tweets = all_tweets_labeled[:TRAINING_SET_SIZE]\n",
    "train_tweets = all_tweets_labeled[TRAINING_SET_SIZE:]\n",
    "\n",
    "###And for those getting controlled later\n",
    "labeled_control_tweets_ut = [(tweet, 'control') for tweet in untokenizedControl]\n",
    "labeled_offensive_tweets_ut = [(tweet, 'offensive') for tweet in untokenizedOffensive]\n",
    "labeled_hateful_tweets_ut = [(tweet, 'hateful') for tweet in untokenizedHate]\n",
    "\n",
    "\n",
    "random.shuffle(labeled_control_tweets_ut)\n",
    "random.shuffle(labeled_offensive_tweets_ut)\n",
    "random.shuffle(labeled_hateful_tweets_ut)\n",
    "\n",
    "all_tweets_untokenized = labeled_hateful_tweets_ut[:1000] + labeled_offensive_tweets_ut[:1000] + labeled_control_tweets_ut[:1000]\n",
    "hate_vs_control_untokenized = labeled_hateful_tweets_ut[:2300] + labeled_control_tweets_ut[:1000]\n",
    "\n",
    "random.shuffle(all_tweets_untokenized)\n",
    "\n",
    "test_list_untokenized = all_tweets_untokenized[:TRAINING_SET_SIZE]\n",
    "train_list_untokenized = all_tweets_untokenized[TRAINING_SET_SIZE:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Cross Valdiation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hate_vs_offensive_tweets = [tweet[0] for tweet in hate_vs_offensive_labeled]\n",
    "hate_vs_offensive_labels = [tweet[1] for tweet in hate_vs_offensive_labeled]\n",
    "\n",
    "print len(hate_vs_offensive_tweets)\n",
    "print len(hate_vs_offensive_labels)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(hate_vs_offensive_tweets,hate_vs_offensive_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working on the classifer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_features = get_word_features(get_words_in_tweets(train_tweets))\n",
    "\n",
    "textList = []\n",
    "labels = []\n",
    "\n",
    "for tweet in train_tweets:\n",
    "    textList.append(tweet[0])\n",
    "    labels.append(tweet[1])\n",
    "\n",
    "print len(textList)\n",
    "print len(labels)\n",
    "\n",
    "\n",
    "print textList[1]\n",
    "print word_features[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###Taken from Larent Lucent (See citation at top of document)\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains{%s}' % word] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "training_set = nltk.classify.apply_features(extract_features, train_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Version!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#classifier = nltk.maxent.ConditionalExponentialClassifier.train(training_set)\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "#classifier = nltk.DecisionTreeClassifier.train(training_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Use a SKLearn Wrapper!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This currently struggles when run locally\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "#SKlearn Wrapper\n",
    "classifier = SklearnClassifier(LinearSVC())\n",
    "    \n",
    "classifier.fit(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "predicted_labels = [classifier.classify(extract_features(tweet[0])) for tweet in test_tweets]\n",
    "\n",
    "actual_labels = [tweet[1] for tweet in test_tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import zero_one_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "\n",
    "print zero_one_loss(actual_labels, predicted_labels)\n",
    "print accuracy_score(actual_labels, predicted_labels)\n",
    "print confusion_matrix(actual_labels, predicted_labels)\n",
    "\n",
    "print f1_score(actual_labels, predicted_labels)\n",
    "\n",
    "print actual_labels\n",
    "print predicted_labels\n",
    "\n",
    "#Necessary for f1_score in binary\n",
    "'''\n",
    "pList = []\n",
    "for label in predicted_labels:\n",
    "    if label is \"offensive\":\n",
    "        pList.append(0)\n",
    "    else:\n",
    "        pList.append(1)\n",
    "        \n",
    "aList = []\n",
    "for label in actual_labels:\n",
    "    if label is \"offensive\":\n",
    "        aList.append(0)\n",
    "    else:\n",
    "        aList.append(1)\n",
    "\n",
    "print f1_score(aList, pList)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "    \n",
    "    \n",
    "print train_list_untokenized[0]\n",
    "cl = NaiveBayesClassifier(train_list_untokenized)\n",
    "\n",
    "print cl.classify(\"I love Christmas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just kidding, that breaks my computer. RIP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
